{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "db0e08a8-c6e1-405f-b412-e451a418f5a1",
      "metadata": {
        "id": "db0e08a8-c6e1-405f-b412-e451a418f5a1",
        "outputId": "a127c359-0f1c-4882-ae83-b216ffc63a0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Output directory set to: G:\\.shortcut-targets-by-id\\1MUpwBt8F3Rg0Vkg71wKmZ-cZdV-9kRr7\\NLP Project\\TRAIN_RELEASE_3SEP2025\\splits_subtask1\n",
            "‚è≥ Loading raw data...\n",
            "Total Users: 137\n",
            "   - Seen Users: 110 (Will have Train/Val/Test history)\n",
            "   - Unseen Users: 27 (Completely held out)\n",
            "üíæ Saving splits...\n",
            "‚úÖ Done! Files created:\n",
            "   - G:\\.shortcut-targets-by-id\\1MUpwBt8F3Rg0Vkg71wKmZ-cZdV-9kRr7\\NLP Project\\TRAIN_RELEASE_3SEP2025\\splits_subtask1\\train.csv (1859 rows)\n",
            "   - G:\\.shortcut-targets-by-id\\1MUpwBt8F3Rg0Vkg71wKmZ-cZdV-9kRr7\\NLP Project\\TRAIN_RELEASE_3SEP2025\\splits_subtask1\\val.csv (235 rows)\n",
            "   - G:\\.shortcut-targets-by-id\\1MUpwBt8F3Rg0Vkg71wKmZ-cZdV-9kRr7\\NLP Project\\TRAIN_RELEASE_3SEP2025\\splits_subtask1\\test_seen.csv (282 rows)\n",
            "   - G:\\.shortcut-targets-by-id\\1MUpwBt8F3Rg0Vkg71wKmZ-cZdV-9kRr7\\NLP Project\\TRAIN_RELEASE_3SEP2025\\splits_subtask1\\test_unseen.csv (388 rows)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# === CONFIGURATION ===\n",
        "# Your exact specific path\n",
        "BASE_DIR = Path(r\"G:\\.shortcut-targets-by-id\\1MUpwBt8F3Rg0Vkg71wKmZ-cZdV-9kRr7\\NLP Project\\TRAIN_RELEASE_3SEP2025\")\n",
        "RAW_DATA_PATH = BASE_DIR / \"train_subtask1.csv\"\n",
        "SPLIT_OUTPUT_DIR = BASE_DIR / \"splits_subtask1\"\n",
        "\n",
        "# Create the output directory if it doesn't exist\n",
        "SPLIT_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"üìÇ Output directory set to: {SPLIT_OUTPUT_DIR}\")\n",
        "\n",
        "# === SPLITTING LOGIC ===\n",
        "def create_splits():\n",
        "    print(\"‚è≥ Loading raw data...\")\n",
        "    df = pd.read_csv(RAW_DATA_PATH)\n",
        "    df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
        "\n",
        "    # 1. Identify Users\n",
        "    user_ids = df['user_id'].unique()\n",
        "    np.random.seed(42)  # Fixed seed for reproducibility\n",
        "    np.random.shuffle(user_ids)\n",
        "\n",
        "    # 2. Split Users: 20% Unseen (Test B), 80% Seen\n",
        "    n_unseen = int(len(user_ids) * 0.2)\n",
        "    unseen_users = user_ids[:n_unseen]\n",
        "    seen_users = user_ids[n_unseen:]\n",
        "\n",
        "    print(f\"Total Users: {len(user_ids)}\")\n",
        "    print(f\"   - Seen Users: {len(seen_users)} (Will have Train/Val/Test history)\")\n",
        "    print(f\"   - Unseen Users: {len(unseen_users)} (Completely held out)\")\n",
        "\n",
        "    # 3. Create 'Unseen' Test Set\n",
        "    df_test_unseen = df[df['user_id'].isin(unseen_users)].copy()\n",
        "\n",
        "    # 4. Process 'Seen' Users (Train -> Val -> Test A)\n",
        "    train_list, val_list, test_seen_list = [], [], []\n",
        "\n",
        "    for uid in seen_users:\n",
        "        # Sort strictly by time\n",
        "        user_df = df[df['user_id'] == uid].sort_values('timestamp')\n",
        "        n = len(user_df)\n",
        "\n",
        "        # 80% Train, 10% Val, 10% Test (Forecasting)\n",
        "        idx_train = int(n * 0.8)\n",
        "        idx_val = int(n * 0.9)\n",
        "\n",
        "        train_list.append(user_df.iloc[:idx_train])\n",
        "        val_list.append(user_df.iloc[idx_train:idx_val])\n",
        "        test_seen_list.append(user_df.iloc[idx_val:])\n",
        "\n",
        "    df_train = pd.concat(train_list)\n",
        "    df_val = pd.concat(val_list)\n",
        "    df_test_seen = pd.concat(test_seen_list)\n",
        "\n",
        "    # === SAVE TO NEW FOLDER ===\n",
        "    print(\"üíæ Saving splits...\")\n",
        "    df_train.to_csv(SPLIT_OUTPUT_DIR / \"train.csv\", index=False)\n",
        "    df_val.to_csv(SPLIT_OUTPUT_DIR / \"val.csv\", index=False)\n",
        "    df_test_seen.to_csv(SPLIT_OUTPUT_DIR / \"test_seen.csv\", index=False)\n",
        "    df_test_unseen.to_csv(SPLIT_OUTPUT_DIR / \"test_unseen.csv\", index=False)\n",
        "\n",
        "    print(\"‚úÖ Done! Files created:\")\n",
        "    print(f\"   - {SPLIT_OUTPUT_DIR / 'train.csv'} ({len(df_train)} rows)\")\n",
        "    print(f\"   - {SPLIT_OUTPUT_DIR / 'val.csv'} ({len(df_val)} rows)\")\n",
        "    print(f\"   - {SPLIT_OUTPUT_DIR / 'test_seen.csv'} ({len(df_test_seen)} rows)\")\n",
        "    print(f\"   - {SPLIT_OUTPUT_DIR / 'test_unseen.csv'} ({len(df_test_unseen)} rows)\")\n",
        "\n",
        "create_splits()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "92c8b757-5c2a-4f41-8234-bb1e9c55d8f6",
      "metadata": {
        "id": "92c8b757-5c2a-4f41-8234-bb1e9c55d8f6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# 1. Create a Global User Mapper from your TRAIN set only\n",
        "train_df = pd.read_csv(SPLIT_OUTPUT_DIR / \"train.csv\")\n",
        "KNOWN_USER_IDS = train_df['user_id'].unique().tolist()\n",
        "# Map real ID -> 1...N.  0 is reserved for \"Unknown/Unseen\"\n",
        "USER_TO_IDX = {uid: i+1 for i, uid in enumerate(KNOWN_USER_IDS)}\n",
        "\n",
        "def get_user_idx(real_user_id):\n",
        "    \"\"\"Returns the trained index for a user, or 0 if unseen.\"\"\"\n",
        "    return USER_TO_IDX.get(real_user_id, 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3cafab00-4ea8-4109-a0d5-29ec6289b30c",
      "metadata": {
        "id": "3cafab00-4ea8-4109-a0d5-29ec6289b30c"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset\n",
        "\n",
        "class TestSlidingWindowDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_length=256, seq_length=5):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "        # Only process users who actually have data in this split\n",
        "        self.user_ids = df[\"user_id\"].unique()\n",
        "        self.df = df\n",
        "\n",
        "        self.samples = []\n",
        "        self._build_samples()\n",
        "\n",
        "    def _build_samples(self):\n",
        "        # Group by user to ensure we don't mix different people's history\n",
        "        for user_id in self.user_ids:\n",
        "            # Sort by time strictly\n",
        "            user_df = self.df[self.df[\"user_id\"] == user_id].sort_values(\"timestamp\").reset_index(drop=True)\n",
        "\n",
        "            texts = user_df[\"text_cleaned\"].fillna(\"\").tolist() # Handle NaNs if any\n",
        "            text_ids = user_df[\"text_id\"].tolist()\n",
        "            valences = user_df[\"valence\"].tolist()\n",
        "            arousals = user_df[\"arousal\"].tolist()\n",
        "\n",
        "            # We must predict for EVERY text in this user's sequence\n",
        "            for i in range(len(texts)):\n",
        "                # Logic: We need the current text + previous 4 texts\n",
        "                start_idx = max(0, i - self.seq_length + 1)\n",
        "                window_texts = texts[start_idx : i + 1]\n",
        "\n",
        "                # Padding: If we are at the start (e.g. 1st text), pad the left with empty strings\n",
        "                if len(window_texts) < self.seq_length:\n",
        "                    pad_len = self.seq_length - len(window_texts)\n",
        "                    window_texts = [\"\"] * pad_len + window_texts\n",
        "\n",
        "                self.samples.append({\n",
        "                    \"texts\": window_texts,\n",
        "                    \"target_valence\": valences[i],\n",
        "                    \"target_arousal\": arousals[i],\n",
        "                    \"text_id\": text_ids[i],\n",
        "                    \"real_user_id\": user_id\n",
        "                })\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.samples[idx]\n",
        "\n",
        "        input_ids_list = []\n",
        "        attention_masks_list = []\n",
        "\n",
        "        for text in item[\"texts\"]:\n",
        "            enc = self.tokenizer(\n",
        "                text,\n",
        "                max_length=self.max_length,\n",
        "                padding=\"max_length\",\n",
        "                truncation=True,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            input_ids_list.append(enc[\"input_ids\"].squeeze(0))\n",
        "            attention_masks_list.append(enc[\"attention_mask\"].squeeze(0))\n",
        "\n",
        "        # HANDLE USER ID MAPPING HERE\n",
        "        # If the user is unseen, this returns 0\n",
        "        mapped_user_idx = get_user_idx(item[\"real_user_id\"])\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.stack(input_ids_list),\n",
        "            \"attention_mask\": torch.stack(attention_masks_list),\n",
        "            \"user_id\": torch.tensor(mapped_user_idx, dtype=torch.long),\n",
        "            \"valence\": torch.tensor(item[\"target_valence\"], dtype=torch.float),\n",
        "            \"arousal\": torch.tensor(item[\"target_arousal\"], dtype=torch.float),\n",
        "            \"text_id\": item[\"text_id\"]\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f3107bad-3332-488b-ae21-a77078b5e51c",
      "metadata": {
        "id": "f3107bad-3332-488b-ae21-a77078b5e51c",
        "outputId": "91810a71-3b1e-41de-9a38-5f2a4758ecdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model will handle 111 user embeddings (Index 0 = Unknown/Unseen)\n"
          ]
        },
        {
          "ename": "NameError",
          "evalue": "name 'EmotionPredictionModel' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[4], line 16\u001b[0m\n\u001b[0;32m     12\u001b[0m NUM_TOTAL_USERS \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(KNOWN_USER_IDS) \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel will handle \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNUM_TOTAL_USERS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m user embeddings (Index 0 = Unknown/Unseen)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mEmotionPredictionModel\u001b[49m(num_users\u001b[38;5;241m=\u001b[39mNUM_TOTAL_USERS)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# 3. Important: When creating datasets, use the TestSlidingWindowDataset for Val and Test\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;66;03m#    (This ensures accurate evaluation)\u001b[39;00m\n\u001b[0;32m     20\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m TestSlidingWindowDataset(pd\u001b[38;5;241m.\u001b[39mread_csv(VAL_PATH), tokenizer)\n",
            "\u001b[1;31mNameError\u001b[0m: name 'EmotionPredictionModel' is not defined"
          ]
        }
      ],
      "source": [
        "# === UPDATED CONFIG FOR YOUR LAPTOP ===\n",
        "\n",
        "# 1. Point to the new splits\n",
        "DATA_DIR = Path(r\"G:\\.shortcut-targets-by-id\\1MUpwBt8F3Rg0Vkg71wKmZ-cZdV-9kRr7\\NLP Project\\TRAIN_RELEASE_3SEP2025\\splits_subtask1\")\n",
        "TRAIN_PATH = DATA_DIR / \"train.csv\"\n",
        "VAL_PATH = DATA_DIR / \"val.csv\"\n",
        "TEST_SEEN_PATH = DATA_DIR / \"test_seen.csv\"\n",
        "TEST_UNSEEN_PATH = DATA_DIR / \"test_unseen.csv\"\n",
        "\n",
        "# 2. Adjust Model Initialization for Unknown Users\n",
        "# +1 accounts for the 0-index we reserved for Unseen Users\n",
        "NUM_TOTAL_USERS = len(KNOWN_USER_IDS) + 1\n",
        "\n",
        "print(f\"Model will handle {NUM_TOTAL_USERS} user embeddings (Index 0 = Unknown/Unseen)\")\n",
        "\n",
        "model = EmotionPredictionModel(num_users=NUM_TOTAL_USERS).to(device)\n",
        "\n",
        "# 3. Important: When creating datasets, use the TestSlidingWindowDataset for Val and Test\n",
        "#    (This ensures accurate evaluation)\n",
        "val_dataset = TestSlidingWindowDataset(pd.read_csv(VAL_PATH), tokenizer)\n",
        "test_seen_dataset = TestSlidingWindowDataset(pd.read_csv(TEST_SEEN_PATH), tokenizer)\n",
        "test_unseen_dataset = TestSlidingWindowDataset(pd.read_csv(TEST_UNSEEN_PATH), tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7fe221e9-7954-4efc-98d5-6d68f9a67b83",
      "metadata": {
        "id": "7fe221e9-7954-4efc-98d5-6d68f9a67b83",
        "outputId": "b6495550-4260-471d-8ad2-1abf5c1e8bd7"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'transformers'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnn\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtransformers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AutoModel\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# === MODEL CONFIGURATION (Must match what we used before) ===\u001b[39;00m\n\u001b[0;32m      6\u001b[0m MODEL_NAME \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdistilbert-base-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'transformers'"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "\n",
        "# === MODEL CONFIGURATION (Must match what we used before) ===\n",
        "MODEL_NAME = \"distilbert-base-uncased\"\n",
        "LSTM_HIDDEN = 128\n",
        "USER_EMBED_DIM = 32\n",
        "\n",
        "class EmotionPredictionModel(nn.Module):\n",
        "    \"\"\"\n",
        "    BERT encoder + LSTM temporal modeling + user embeddings,\n",
        "    with separate regression heads for valence & arousal.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_users: int,\n",
        "        bert_model: str = MODEL_NAME,\n",
        "        lstm_hidden: int = LSTM_HIDDEN,\n",
        "        user_embed_dim: int = USER_EMBED_DIM,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        # 1. Pretrained BERT encoder\n",
        "        self.bert = AutoModel.from_pretrained(bert_model)\n",
        "        bert_dim = self.bert.config.hidden_size\n",
        "\n",
        "        # 2. User embeddings (The \"ID Card\" for every user)\n",
        "        self.user_embedding = nn.Embedding(num_users, user_embed_dim)\n",
        "\n",
        "        # 3. LSTM over sequences (The \"Time Traveler\")\n",
        "        # Input = BERT vector (768) + User vector (32) = 800\n",
        "        self.lstm = nn.LSTM(\n",
        "            input_size=bert_dim + user_embed_dim,\n",
        "            hidden_size=lstm_hidden,\n",
        "            num_layers=1,\n",
        "            batch_first=True,\n",
        "            bidirectional=True,\n",
        "            dropout=0.1,\n",
        "        )\n",
        "\n",
        "        # 4. Regression heads (The \"Decision Makers\")\n",
        "        # Input is LSTM_HIDDEN * 2 because it is bidirectional\n",
        "        self.valence_head = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden * 2, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 1),\n",
        "        )\n",
        "\n",
        "        self.arousal_head = nn.Sequential(\n",
        "            nn.Linear(lstm_hidden * 2, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.3),\n",
        "            nn.Linear(128, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, user_id):\n",
        "        \"\"\"\n",
        "        input_ids: [batch, seq_len, max_len]\n",
        "        \"\"\"\n",
        "        batch_size, seq_len, max_len = input_ids.shape\n",
        "\n",
        "        # A. Encode text with BERT\n",
        "        # Flatten to [batch*seq_len, max_len] because BERT takes 2D inputs\n",
        "        input_ids_flat = input_ids.view(-1, max_len)\n",
        "        attn_mask_flat = attention_mask.view(-1, max_len)\n",
        "\n",
        "        bert_outputs = self.bert(input_ids_flat, attention_mask=attn_mask_flat)\n",
        "        cls_embeds = bert_outputs.last_hidden_state[:, 0, :]          # [batch*seq_len, hidden]\n",
        "        text_embeds = cls_embeds.view(batch_size, seq_len, -1)        # [batch, seq_len, hidden]\n",
        "\n",
        "        # B. Add User Embeddings\n",
        "        # Expand user_id from [batch] to [batch, seq_len, 1]\n",
        "        user_embeds = self.user_embedding(user_id).unsqueeze(1).expand(-1, seq_len, -1)\n",
        "\n",
        "        # Combine Text + User\n",
        "        combined = torch.cat([text_embeds, user_embeds], dim=-1)      # [batch, seq_len, 800]\n",
        "\n",
        "        # C. Run LSTM\n",
        "        lstm_out, _ = self.lstm(combined)                             # [batch, seq_len, 256]\n",
        "\n",
        "        # D. Predict\n",
        "        valence = self.valence_head(lstm_out).squeeze(-1)            # [batch, seq_len]\n",
        "        arousal = self.arousal_head(lstm_out).squeeze(-1)            # [batch, seq_len]\n",
        "\n",
        "        return valence, arousal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74a91a50-7ce3-44d9-8e58-8b619d8720b9",
      "metadata": {
        "id": "74a91a50-7ce3-44d9-8e58-8b619d8720b9",
        "outputId": "97552f34-6dca-4fe8-f578-707b82f3cada"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:‚úÖ Loaded 110 users from disk.\n",
            "INFO:__main__:‚è≥ Loading Best Model...\n",
            "C:\\Users\\hasee\\miniconda3\\envs\\torch-env\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:123: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  warnings.warn(\n",
            "C:\\Users\\hasee\\AppData\\Local\\Temp\\ipykernel_28432\\1562346025.py:112: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  best_model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
            "INFO:__main__:‚úÖ Model Weights Loaded Successfully!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# === 1. CONFIGURATION ===\n",
        "BATCH_SIZE = 8           # Safe for Inference on 8GB VRAM\n",
        "SEQ_LENGTH = 10\n",
        "MAX_SEQ_LEN = 256\n",
        "BASE_DIR = Path(r\"G:\\.shortcut-targets-by-id\\1MUpwBt8F3Rg0Vkg71wKmZ-cZdV-9kRr7\\NLP Project\\TRAIN_RELEASE_3SEP2025\")\n",
        "SPLIT_OUTPUT_DIR = BASE_DIR / \"splits_subtask1\"\n",
        "BEST_MODEL_PATH = \"best_model.pt\"\n",
        "\n",
        "# Device Setup\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "logging.basicConfig(level=logging.INFO)\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# === 2. RE-DEFINE CLASSES (Must match training exactly) ===\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
        "\n",
        "# Reconstruct User Mapping\n",
        "if (SPLIT_OUTPUT_DIR / \"train.csv\").exists():\n",
        "    train_df_temp = pd.read_csv(SPLIT_OUTPUT_DIR / \"train.csv\")\n",
        "    KNOWN_USER_IDS = train_df_temp['user_id'].unique().tolist()\n",
        "    USER_TO_IDX = {uid: i+1 for i, uid in enumerate(KNOWN_USER_IDS)}\n",
        "    logger.info(f\"‚úÖ Loaded {len(KNOWN_USER_IDS)} users from disk.\")\n",
        "else:\n",
        "    raise FileNotFoundError(\"Splits not found!\")\n",
        "\n",
        "def get_user_idx(real_id): return USER_TO_IDX.get(real_id, 0)\n",
        "\n",
        "class EmotionDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, seq_length=10, is_test=False):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.seq_length = seq_length\n",
        "        self.samples = []\n",
        "        for uid in df['user_id'].unique():\n",
        "            u_df = df[df['user_id'] == uid].sort_values('timestamp').reset_index(drop=True)\n",
        "            texts = u_df[\"text_cleaned\"].fillna(\"\").tolist()\n",
        "\n",
        "            # For Test/Eval: Predict for EVERY text using padding\n",
        "            # For Train: Only full sequences\n",
        "            iterator = range(len(texts)) if is_test else range(len(texts) - seq_length + 1)\n",
        "\n",
        "            for i in iterator:\n",
        "                if is_test:\n",
        "                    start = max(0, i - seq_length + 1)\n",
        "                    window_texts = texts[start : i+1]\n",
        "                    if len(window_texts) < seq_length:\n",
        "                        window_texts = [\"\"] * (seq_length - len(window_texts)) + window_texts\n",
        "\n",
        "                    self.samples.append({\n",
        "                        \"texts\": window_texts,\n",
        "                        \"v\": u_df.iloc[i][\"valence\"], \"a\": u_df.iloc[i][\"arousal\"],\n",
        "                        \"uid\": uid\n",
        "                    })\n",
        "                else:\n",
        "                    if len(texts) < seq_length: continue\n",
        "                    window = u_df.iloc[i : i+seq_length]\n",
        "                    self.samples.append({\n",
        "                        \"texts\": window[\"text_cleaned\"].fillna(\"\").tolist(),\n",
        "                        \"v\": window[\"valence\"].tolist(), \"a\": window[\"arousal\"].tolist(),\n",
        "                        \"uid\": uid\n",
        "                    })\n",
        "\n",
        "    def __len__(self): return len(self.samples)\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.samples[idx]\n",
        "        input_ids, masks = [], []\n",
        "        for text in item[\"texts\"]:\n",
        "            enc = self.tokenizer(text, max_length=MAX_SEQ_LEN, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n",
        "            input_ids.append(enc[\"input_ids\"].squeeze(0))\n",
        "            masks.append(enc[\"attention_mask\"].squeeze(0))\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": torch.stack(input_ids),\n",
        "            \"attention_mask\": torch.stack(masks),\n",
        "            \"user_id\": torch.tensor(get_user_idx(item[\"uid\"]), dtype=torch.long),\n",
        "            \"valence\": torch.tensor(item[\"v\"], dtype=torch.float),\n",
        "            \"arousal\": torch.tensor(item[\"a\"], dtype=torch.float)\n",
        "        }\n",
        "\n",
        "class EmotionModel(nn.Module):\n",
        "    def __init__(self, num_users):\n",
        "        super().__init__()\n",
        "        self.bert = AutoModel.from_pretrained(\"distilbert-base-uncased\")\n",
        "        self.user_emb = nn.Embedding(num_users, 32)\n",
        "        self.lstm = nn.LSTM(768+32, 128, batch_first=True, bidirectional=True, dropout=0.1)\n",
        "        self.head_v = nn.Sequential(nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 1))\n",
        "        self.head_a = nn.Sequential(nn.Linear(256, 64), nn.ReLU(), nn.Linear(64, 1))\n",
        "\n",
        "    def forward(self, input_ids, mask, uid):\n",
        "        b, s, m = input_ids.shape\n",
        "        bert_out = self.bert(input_ids.view(-1, m), mask.view(-1, m)).last_hidden_state[:, 0, :]\n",
        "        text_emb = bert_out.view(b, s, -1)\n",
        "        user_emb = self.user_emb(uid).unsqueeze(1).expand(-1, s, -1)\n",
        "        lstm_out, _ = self.lstm(torch.cat([text_emb, user_emb], dim=-1))\n",
        "        return self.head_v(lstm_out).squeeze(-1), self.head_a(lstm_out).squeeze(-1)\n",
        "\n",
        "# === 3. LOAD THE TRAINED WEIGHTS ===\n",
        "logger.info(\"‚è≥ Loading Best Model...\")\n",
        "best_model = EmotionModel(len(KNOWN_USER_IDS) + 1).to(device)\n",
        "\n",
        "if os.path.exists(BEST_MODEL_PATH):\n",
        "    best_model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
        "    logger.info(\"‚úÖ Model Weights Loaded Successfully!\")\n",
        "else:\n",
        "    logger.error(\"‚ùå best_model.pt not found! Did you delete it?\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7f84e3ee-c28d-4047-9582-3de9d97c0b8e",
      "metadata": {
        "id": "7f84e3ee-c28d-4047-9582-3de9d97c0b8e",
        "outputId": "deaaaf7b-048a-4920-e3c4-82617175d8a8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:__main__:üîé Evaluating on TEST_SEEN...\n",
            "Predicting TEST_SEEN: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 35/35 [00:10<00:00,  3.24it/s]\n",
            "INFO:__main__:üìä TEST_SEEN RESULTS:\n",
            "INFO:__main__:   Valence Correlation: 0.7026\n",
            "INFO:__main__:   Arousal Correlation: 0.5186\n",
            "INFO:__main__:üíæ Saved to predictions\\pred_TEST_SEEN.csv\n",
            "\n",
            "INFO:__main__:üîé Evaluating on TEST_UNSEEN...\n",
            "Predicting TEST_UNSEEN: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 60/60 [00:18<00:00,  3.24it/s]\n",
            "INFO:__main__:üìä TEST_UNSEEN RESULTS:\n",
            "INFO:__main__:   Valence Correlation: 0.6386\n",
            "INFO:__main__:   Arousal Correlation: 0.4241\n",
            "INFO:__main__:üíæ Saved to predictions\\pred_TEST_UNSEEN.csv\n",
            "\n",
            "INFO:__main__:‚úÖ All done!\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# Directory for results\n",
        "OUTPUT_DIR = Path(\"predictions\")\n",
        "OUTPUT_DIR.mkdir(exist_ok=True)\n",
        "\n",
        "def run_evaluation(model, dataset_path, split_name):\n",
        "    logger.info(f\"üîé Evaluating on {split_name}...\")\n",
        "\n",
        "    # Load Data with is_test=True (Ensures we predict for EVERY text)\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    ds = EmotionDataset(df, tokenizer, seq_length=SEQ_LENGTH, is_test=True)\n",
        "    loader = DataLoader(ds, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "    model.eval()\n",
        "    all_preds_v, all_preds_a = [], []\n",
        "    all_true_v, all_true_a = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(loader, desc=f\"Predicting {split_name}\"):\n",
        "            input_ids = batch[\"input_ids\"].to(device)\n",
        "            mask = batch[\"attention_mask\"].to(device)\n",
        "            uid = batch[\"user_id\"].to(device)\n",
        "\n",
        "            # Forward Pass\n",
        "            out_v, out_a = model(input_ids, mask, uid)\n",
        "\n",
        "            # We only care about the LAST item in the sequence for inference\n",
        "            last_v = out_v[:, -1].cpu().numpy()\n",
        "            last_a = out_a[:, -1].cpu().numpy()\n",
        "\n",
        "            all_preds_v.extend(last_v)\n",
        "            all_preds_a.extend(last_a)\n",
        "            all_true_v.extend(batch[\"valence\"].numpy())\n",
        "            all_true_a.extend(batch[\"arousal\"].numpy())\n",
        "\n",
        "    # Metrics\n",
        "    corr_v, _ = pearsonr(all_true_v, all_preds_v)\n",
        "    corr_a, _ = pearsonr(all_true_a, all_preds_a)\n",
        "\n",
        "    logger.info(f\"üìä {split_name} RESULTS:\")\n",
        "    logger.info(f\"   Valence Correlation: {corr_v:.4f}\")\n",
        "    logger.info(f\"   Arousal Correlation: {corr_a:.4f}\")\n",
        "\n",
        "    # Save to CSV\n",
        "    result_df = pd.DataFrame({\n",
        "        \"true_valence\": all_true_v, \"pred_valence\": all_preds_v,\n",
        "        \"true_arousal\": all_true_a, \"pred_arousal\": all_preds_a\n",
        "    })\n",
        "    result_df.to_csv(OUTPUT_DIR / f\"pred_{split_name}.csv\", index=False)\n",
        "    logger.info(f\"üíæ Saved to {OUTPUT_DIR / f'pred_{split_name}.csv'}\\n\")\n",
        "\n",
        "# Run Evaluation\n",
        "run_evaluation(best_model, SPLIT_OUTPUT_DIR / \"test_seen.csv\", \"TEST_SEEN\")\n",
        "run_evaluation(best_model, SPLIT_OUTPUT_DIR / \"test_unseen.csv\", \"TEST_UNSEEN\")\n",
        "\n",
        "logger.info(\"‚úÖ All done!\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}